import os
import numpy as np
import scipy as sp
import pickle
import warnings
import math
from datetime import datetime

from spatial import calc_place_cells

from . import behavior as behav
from . import preprocessing as pp

from sessions_dict import single_plane, multi_plane

all_sess = single_plane
all_sess.update(multi_plane)

# Set dff defaults for collecting sess data across animals
default_dff_method = {
    "neuropil_method_red": "subtract",
    "neuropil_method_green": None,
    "regress_g_from_r": True,
    "regress_r_from_g": False,
    "baseline_method": "maximin",
    "neu_coef": 0.7,
    "keep_teleports": False,
}

def print_full():
    """ Quick function to print full numpy arrays """
    import sys
    np.set_printoptions(threshold=sys.maxsize)
    return

def make_anim_tag(anim_list):
    """ Make a string of animal numbers, connected by hyphens """
    anim_tag = "-".join([get_mouse_number(s) for s in anim_list])

    return anim_tag

def make_day_tag(day_list):
    """ Make a string of day numbers, connected by hyphens """
    day_str = "-".join([str(d) for d in day_list])
    return day_str

def make_fig_dir(path_dict):    
    
    """ make a month-year figure directory to save figures """
    myyyy = datetime.now().strftime("%b") + datetime.now().strftime("%Y")
    fig_dir = os.path.join(path_dict['fig_dir'],myyyy)
    os.makedirs(fig_dir,exist_ok=True)
    print(fig_dir)
    
    return fig_dir

def make_date_string():
    
    return datetime.now().strftime("%Y%m%d-%H%M")


def write_sess_pickle(sess, sess_dir, pkl_name, overwrite=False):
    """
    Write a sess class to a .pickle file

    :param sess: session class
    :param sess_dir: directory to write the pickle file
    :param pkl_name: pickle file name
    :param overwrite: whether to overwrite an existing pickle file with this name, if it exists
    :return:
    """

    print("writing", pkl_name)

    if overwrite:
        save_sess = open(os.path.join(sess_dir, pkl_name), "wb")
        pickle.dump(sess, save_sess)
    else:
        if os.path.exists(os.path.join(sess_dir, pkl_name)):
            raise NotImplementedError(
                ".pickle already exists, aborting save. Set overwrite=True to overwrite."
            )
        else:
            save_sess = open(os.path.join(sess_dir, pkl_name), "wb")
            pickle.dump(sess, save_sess)
            # Close the pickle
    save_sess.close()

    return


def load_sess_pickle(basedir, animal, day=None, exp_day=None):
    """
    Load sess class from existing pickle file

    :param basedir: preprocessed 2P data directory
    :param animal: animal name
    :param day: day index out of imaging days (0-indexed)
    :param exp_day: experimental day (1-indexed)
    :return: sess
    """
    if day is None and exp_day is None:
        raise NotImplementedError("You must specify either a day (index) or exp day")

    pkl_path = get_sess_pkl_path(basedir, animal, day=day, exp_day=exp_day)
    print(pkl_path)

    sess = pickle.load(open(pkl_path, "rb"))

    return sess


def quick_load_multi_anim_sess(day, experiment='MetaLearn', anim_list=None, params=None):
    """ Quickly load pre-saved, processed data, assuming some file paths exist """

    # experiment = 'MetaLearn' # 'NeuroMods' or 'MetaLearn' of 'EnvSwitch'
    from reward_relative import dayData as dd
    from reward_relative.path_dict_firebird import path_dictionary as path_dict
    
    if anim_list is None:
        anim_list = dd.define_anim_list(experiment, day, 'combined')
        
    if params is None:
        print("loading multi anim sess with default params")
        params = {'speed_thr': 2, # speed threshold for place cells
                  'nperms': 100,
                  'output_shuffle': False,
                  'shuffle_method': 'population',
                  'p_thr':  0.05, # p value threshold
                  'stability_thr': None,
                  'ts_key': 'events', #timeseries to use for place cell detection
                  'trial_subsets': True,
                  'min_subtract': False,
                 }
        
    max_anim_tag = "-".join([get_mouse_number(s) for s in anim_list])

    baseline_method = 'maximin'
    shuffle_tag = f'_{baseline_method}' #"_w-shuffle"
    spd = '2'
    perms = 100
    all_anim = {}
    pkl_path = os.path.join(path_dict['preprocessed_root'],'multi_anim_sess',
                           ('%s_expday%d_speed%s_perms%d_%s_%s.pickle' % (max_anim_tag, 
                                                                     day,
                                                                     str(params['speed_thr']),
                                                                     params['nperms'],
                                                                     baseline_method,
                                                                       params['ts_key'])
                           )
                           )
                            #('%s_expday%d_speed%s_perms%d%s.pickle' % (max_anim_tag,exp_day,spd,perms,shuffle_tag)))
    print(pkl_path)
    all_anim = pickle.load(open(pkl_path,"rb"))
    
    return all_anim


def get_sess_pkl_path(basedir, animal, day=None, exp_day=None):
    """
    Generate path for sess pickle file

    :param basedir: preprocessed 2P data directory
    :param animal: animal name
    :param day: day index out of imaging days (0-indexed)
    :param exp_day: experimental day (1-indexed)
    :return:
    """

    if day is None and exp_day is None:
        raise NotImplementedError("You must specify either a day (index) or exp day")

    if day is not None:
        # !! Hack!! for now, just take the first session if there are multiple
        # TO DO: concatenate and load multiple sessions
        if type(all_sess[animal][day]) is tuple:
            print(
                "There are multiple sessions for this day -- currently giving only the 1st session"
            )
            this_sess = all_sess[animal][day][0]
        else:
            this_sess = all_sess[animal][day]

    if exp_day is not None:
        this_sess = get_sess_for_exp_day(all_sess, animal, exp_day)
        if len(this_sess) > 1:
            print(
                "There are multiple sessions for this day -- currently giving only the 1st session"
            )
        this_sess = this_sess[0]

    date = this_sess["date"]
    scene = this_sess["scene"]
    session = this_sess["session"]
    scan_number = this_sess["scan"]
    
    if np.isnan(scan_number):
        scan_number = 0

    fullpath = os.path.join(
        basedir,
        "sess",
        animal,
        date,
        "%s_%03d_%03d.pickle" % (scene, session, scan_number),
    )

    return fullpath


def get_sess_string_for_exp_day(basedir, animal, exp_day=None):
    """
    Generate string that specifies sess (i.e. for a file name)

    :param basedir: preprocessed 2P data directory
    :param animal: animal name
    :param exp_day: experimental day (1-indexed)
    :return:
    """
    path = dict()

    if exp_day is not None:
        info = get_sess_for_exp_day(all_sess, animal, exp_day)
        if len(info) > 1:
            print("There are multiple sessions for this day")
    else:
        raise NotImplementedError("You must specify an exp day")

    for d, entry in enumerate(info):
        date = info[d]["date"]
        scene = info[d]["scene"]
        session = info[d]["session"]
        scan_number = info[d]["scan"]

        fullpath = os.path.join(
            basedir, animal, date, scene, "%s_%03d_%03d" % (scene, session, scan_number)
        )
        path[d] = fullpath

    return path


def get_sess_for_exp_day(all_sess_dict, animal, exp_day):
    """
    find the session data corresponding to an experiment day number

    :param all_sess_dict: a dictionary of all session data,
            organized by animal and day, defined in sessions_dict.py
    :param animal: animal name (string)
    :param exp_day: experiment day number (int)
    :return:
    """

    info = dict()
    for day_dict in all_sess_dict[animal]:
        if type(day_dict) is tuple:
            for i, entry in enumerate(day_dict):
                # print(i)
                if entry["exp_day"] == exp_day:
                    info[i] = entry
        else:
            if day_dict["exp_day"] == exp_day:
                info[0] = day_dict

    return info


def get_ind_of_exp_day(all_sess_dict, animal, exp_day):
    info = dict()

    for ind, day_dict in enumerate(all_sess_dict[animal]):
        if type(day_dict) is tuple:
            for i, entry in enumerate(day_dict):
                # print(i)
                if entry["exp_day"] == exp_day:
                    index = ind
        else:
            if day_dict["exp_day"] == exp_day:
                index = ind

    return index


def get_mouse_number(s):
    """
    :param s: string, full mouse name
    :return: number at the end of the mouse name
    """

    header = s.rstrip("0123456789")
    return s[len(header):]


def lookup(value, array):
    """
    Lookup the nearest value to "value" in "array"

    :param value: value to look up
    :param array: array in which to look it up
    :return: closest value
    """
    
    idx = lookup_ind(value, array)
    arr = np.asarray(array)

    return arr[idx]


def lookup_ind(value, array):
    """
    Lookup the index of the nearest value to "value" in "array"

    :param value: value to look up
    :param array: array in which to look it up
    :return: index of closest value
    """
    arr = np.asarray(array)
    val = np.asarray(value)
    
    if len(val.ravel())>1:
        idx = [np.nanargmin((np.abs(arr - v))) for v in val.tolist()]
    else:
        idx = np.nanargmin((np.abs(arr - val)))
    return idx

def lookup_ind_bigger(value, array):
    """
    Lookup the index of the nearest value to "value" in "array" that is
    bigger than "value"

    :param value: value to look up
    :param array: array in which to look it up
    :return: index of closest value
    """
    
    arr = np.asarray(array)
    val = np.asarray(value)
    # get signed difference
    diff = (arr - val).astype(float)
    # get rid of negative values
    diff[diff<0] = np.nan
    idx = np.nanargmin((np.abs(diff)))    
    return idx

def lookup_ind_exact(value,array):
    """
    Find index of exact matches to "value" in "array".
    If there are no matches, return NaN.

    :param value: value to look up
    :param array: array in which to look it up
    :return: index of the match
    """
    arr = np.asarray(array)
    val = np.asarray(value)

    inds = np.zeros((len(value),))*np.nan

    for i,v in enumerate(val.tolist()):
        find = np.where(array==v)
        if len(find[0])>0:
            inds[i] = int(find[0])
    
    return inds



def find_contiguous(data, stepsize=1, up_to_stepsize=None):
    """
    Find contiguous runs of elements based on a "stepsize" difference between them.
    Thanks to Stack Exchange for this inspiration

    :param data: array in which to look for contiguous runs
    :param stepsize: target difference between elements
    :return: an array of arrays
    """

    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=np.VisibleDeprecationWarning)
        if up_to_stepsize is not None:
            runs = np.asarray(np.split(data, np.where(np.diff(data) > stepsize)[0] + 1)) #, dtype=object)
        else:
            runs = np.asarray(np.split(data, np.where(np.diff(data) != stepsize)[0] + 1)) #, dtype=object)
        return runs

    
def indep_roll(arr_, shifts_, axis=1):
    """
    Apply an independent roll for each dimensions of a single axis; for example, 
    roll each row of arr_ independently in the columns dimension by the value 
    specified in the corresponding row of shifts_.
    
    Necessary because np.roll does not allow arrays as the shift input.
    A GEM from the depths of stack overflow, thank you user Yann Dubois!

    Parameters
    ----------
    arr : np.ndarray
        Array of any shape.

    shifts : np.ndarray
        How many shifting to use for each dimension. Shape: `(arr.shape[axis],)`.

    axis : int
        Axis along which elements are shifted. 
    """
    arr = np.copy(arr_)
    arr = np.swapaxes(arr,axis,-1)
    all_idcs = np.ogrid[[slice(0,n) for n in arr.shape]]

    # Convert to a positive shift to wrap negative shifts
    shifts = np.copy(shifts_)
    shifts[shifts < 0] += arr.shape[-1] 
    all_idcs[-1] = all_idcs[-1] - shifts[:, np.newaxis]

    result = arr[tuple(all_idcs)]
    arr = np.swapaxes(result,-1,axis)
    return arr


def subsequences(iterable, length):
    """
    Return an iterable object containing consecutive slices of an iterable
    with length n
    """
    seq = [iterable[i: i + length] for i in range(len(iterable) - length + 1)]
    #ensure consistent format
    if type(seq[0]) is not list:
        seq = [s.tolist() for s in seq]
    
    return seq


def intersection(list1, list2):
    """
    Return the intersecting values of 2 lists (values contained in both lists)
    """
    intersect = [value for value in list1 if value in list2]
    return intersect


def center_of_mass(data, coord=None, axis=0):
    """
    Find the data's absolute center of mass (COM) along the given axis

    :param data: mass
    :param coord: coordinates of the data, from which we calculate center of mass
            -- If coord==None, coord are the indices of the data.
    :param axis: axis to calculate across
    :return:
    """

    valid_data = np.copy(data) #[np.where(~np.isnan(data))]

    if np.sum(~np.isnan(valid_data))>0:
        if coord is None:
            coord = np.indices((data.shape))[axis]
            # coord = np.arange(0, data.shape[axis])

        #valid_coord = coord[~np.isnan(data)]

        # make data positive, looking for center of upward-going mass
        mass = valid_data - np.nanmin(valid_data, axis=axis, keepdims=True)

        normalizer = np.nansum(np.abs(valid_data), axis=axis, keepdims=True)

        with np.errstate(invalid='ignore', divide='ignore'):
            COM = (
                    np.nansum(np.abs(valid_data) * coord, axis=axis, keepdims=True) / normalizer
            )
    else:
        COM = np.nan

    return COM

def center_of_mass_signed(weights, coord):
    
    com = np.nansum(weights*coord, axis=-1)/(np.nansum(weights,axis=-1)+1E-5)
    
    return com

def zscore(x, axis=None):
    """
    same as scipy.stats.zscore but automatically ignoring nans

    :param x: input data
    :param axis: axis to calculate mean and std over
    :return:
    """

    z_x = (x - np.nanmean(x, axis=axis, keepdims=True)) / np.nanstd(
        x, axis=axis, keepdims=True
    )

    return z_x


def sem(x, axis=0, **kwargs):
    """
    standard error of the sample mean (using N-1)

    :param x: input data
    :param axis: axis to calculate std over
    :return:
    """

    x_ = np.copy(np.asarray(x))

    return np.nanstd(x_, axis=axis, **kwargs) / np.sqrt(x_.shape[axis] - 1)


def calc_cov_mat(data, N_axis=0):
    """
    Calculate the neuron x neuron (or ROI x ROI) covariance matrix
    covariance = (1/N) * X @ X.T

    :param data: e.g. 2D array of neurons x time points
    :param N_axis: the axis over which to take the number of neurons
    :return:
    """
    cov_mat = (1 / data.shape[N_axis]) * (data @ data.T)
    return cov_mat


def moving_average(x, w, pad_repeats=0):
    """
    Calculate a moving average over a window

    :param x: input data
    :param w: width of moving window in samples (indices)
    :return:
    """
    
    if pad_repeats > 0:
        x_ = np.copy(x)
        x_ = np.insert(x_, 0, np.repeat(x_[0],pad_repeats))
        x_ = np.insert(x_, -1, np.repeat(x_[-1],pad_repeats))
    else:
        x_ = np.copy(x)
        
    ma = np.convolve(x_, np.ones(w), "valid") / w
    
    if pad_repeats > 0:
        ma = ma[pad_repeats:-pad_repeats]
        
    return ma


def nansmooth(a, sig, axis=-1, return_nans=False, **kwargs):
    """
    apply Gaussian smoothing to matrix A containing nans with kernel sig
    without propagating nans
    :param a:
    :param sig:
    :return:
    """

    # find nans
    nan_inds = np.isnan(a)
    a_nanless = np.copy(a)
    # make nans 0
    a_nanless[nan_inds] = 0

    # inversely weight nanned indices
    one = np.ones(a.shape)
    one[nan_inds] = 0.001
    a_nanless = sp.ndimage.filters.gaussian_filter1d(a_nanless, sig, axis=axis, **kwargs)
    one = sp.ndimage.filters.gaussian_filter1d(one, sig, axis=axis, **kwargs)
    
    out = a_nanless / one
    if return_nans:
        out[nan_inds] = np.nan
    
    return out


def nanargmax(a, axis=None, out=None, *, keepdims=np._NoValue):
    """
    Improved numpy.nanargmax where it returns NaN as the result if
    the input is all NaNs
    Note that using np.argmax without replacing the NaNs returns
    the index of the first NaN as the max (which is absurd)!!!!

    """

    a, mask = _replace_nan(a, -np.inf)
    if mask is not None:
        mask = np.all(mask, axis=axis)
        if np.any(mask):
            print("input is all NaNs")
            keepaxis = np.delete(np.arange(len(a.shape)), axis)
            res = np.zeros(tuple(a.shape[ax] for ax in keepaxis)) * np.nan
        else:
            try:
                res = np.argmax(a, axis=axis, out=out, keepdims=keepdims)
            except:
                res = np.argmax(a, axis=axis, out=out)
    else:
        try:
            res = np.argmax(a, axis=axis, out=out, keepdims=keepdims)
        except:
            res = np.argmax(a, axis=axis, out=out)
    return res


def _replace_nan(a, val):
    mask = np.isnan(a)

    if np.any(mask):
        a = np.array(a, subok=True, copy=True)
        np.copyto(a, val, where=mask)

    return a, mask


def round_up(n, decimals=0):
    multiplier = 10**decimals
    return math.ceil(n * multiplier) / multiplier


def find_legs(c):
    # Calculate the lengths of the legs using the Pythagorean Theorem
    a = math.sqrt(c**2 / 2)
    b = math.sqrt(c**2 - a**2)

    return a, b

def permutation_test(_shuffles, _observed):
    """
    Two-sided permutation test relative to shuffle
    with plus-one correction to avoid zero pvalues
    
    :param shuffles: a numpy array of shuffled values
    :param observed: the observed statistic you want to test
    :return: p-value
    """
    # check if observed or shuffles are nan
    observed = np.copy(_observed).astype(float)
    shuffles = np.copy(_shuffles).astype(float)
    
    if ~np.isnan(observed):
        if np.any(~np.isnan(shuffles)):
            if np.any(np.isnan(shuffles)):
                warnings.warn(f"NaNs present in {np.sum(np.isnan(shuffles))} of {len(shuffles)} shuffles")
                      
            p = (np.sum(
                (np.abs(shuffles) >= np.abs(observed))) + 1) / (len(shuffles) +1)
        else:
            p = np.nan
    else:
        p = np.nan
    
    return p


def avoid_naninf(data_):
    
    data = np.copy(data_)
    data[data==1] = np.nan
    data[data==0] = np.nan
    
    return data


def compute_MSE_from_matrix(X, axis=0):
    #model output mean squared error
    sse = np.sum((np.nanmean(X,axis=axis)-X)**2)
    # divide by product of the number of points in each dimension
    mse = (1 / np.product(X.shape)) * sse
    return mse

def compute_SSE_from_matrix(X, axis=0):
    #model output summed squared error
    sse = np.sum((np.nanmean(X,axis=axis)-X)**2)
    return sse

## ------------------------------------------------------------------##
"""
Below is a much larger 'utility' for running preprocessing and storage
of  data. To-do: move to its own module.
"""

def multi_anim_sess(
        path_dict,
        anim_list,
        exp_day,
        calc_dff=True,
        calc_pcs=True,
        calc_spks=False,
        ts_key='events ',
        dff_method=None,
        trial_subsets=False,
        nperms=100,
        output_shuffle=False,
        p_thr=0.05,
        stability_thr=None,
        speed_thr=0,
        **pc_kwargs,
):
    """
    For a single session (experiment day), get sess class,
    dff, place cells, and behavior for multiple animals.

    :param path_dict: path dictionary, including your directory for saved sess
            -- this assumes a dictionary called 'all_session_dict.pkl' is saved
            where in the parent directory of the animal data
    :param anim_list: list of animal names as strings
    :param exp_day: which experiment day (1-indexed)
    :param calc_dff: whether to recalculate dff (default True)
    :param calc_pcs: whether to calculate place cells (default True)
    :param ts_key: timeseries key to use (default dff, as opposed to events)
    :param dff_method: default dictionary for dff_dual params defined above
    :param trial_subsets: whether to calculate place cells from separate subsets of trials
    :param nperms: how many permutations for place cell spatial information significance
    :param kwargs: speed_thr, etc.
    :return: all_anim: a dictionary with the sess for that exp_day of each animal,
                  plus other useful behavioral info
    """

    all_anim = dict()

    if dff_method is None:
        dff_method = dict.copy(default_dff_method)
    else:
        for key,val in default_dff_method.items():
            dff_method[key] = dff_method.get(key, val)

    for an_i, animal in enumerate(anim_list):

        print(f"animal {an_i}: {animal}")

        #         sess_info = get_sess_for_exp_day(all_sess,animal,exp_day)

        # load saved sess class, if you've already run make_session_pkl.ipynb
        pkl_path = get_sess_pkl_path(
            path_dict["preprocessed_root"], animal, exp_day=exp_day
        )
        sess = load_sess_pickle(path_dict["preprocessed_root"], animal, exp_day=exp_day)

        if calc_dff:
            # currently not set up for multiplane
            if len(sess.timeseries["F"].shape) == 3:
                raise NotImplementedError(
                    "Not configured for downsampled F without cells"
                )

            if sess.scan_info["nChan"] > 1:
                # Get df/F
                print("Assigning red as 'F', green as 'F_chan2'")
                F_r = sess.timeseries["F"]
                F_g = sess.timeseries["F_chan2"]

                Fneu_r = sess.timeseries["Fneu"]
                Fneu_g = sess.timeseries["Fneu_chan2"]

                trial_starts = sess.trial_start_inds
                teleports = sess.teleport_inds

                if calc_spks:
                    dFF, dFF2, events = pp.dff_dual(
                        F_r,
                        Fneu_r,
                        F_g,
                        Fneu_g,
                        trial_starts,
                        teleports,
                        neuropil_method_red=dff_method["neuropil_method_red"],
                        neuropil_method_green=dff_method["neuropil_method_green"],
                        baseline_method=dff_method["baseline_method"],
                        regress_g_from_r=dff_method["regress_g_from_r"],
                        regress_r_from_g=dff_method["regress_r_from_g"],
                        neu_coef=dff_method["neu_coef"],
                        tau=sess.s2p_ops["tau"],
                        frame_rate=sess.scan_info["frame_rate"],
                        n_planes=sess.n_planes,
                        deconvolve=True,
                        keep_teleports=dff_method["keep_teleports"],
                    )
                    sess.add_timeseries(**{"events": events})
                    sess.add_pos_binned_trial_matrix("events", "pos")
                else:
                    dFF, dFF2 = pp.dff_dual(
                        F_r,
                        Fneu_r,
                        F_g,
                        Fneu_g,
                        trial_starts,
                        teleports,
                        neuropil_method_red=dff_method["neuropil_method_red"],
                        neuropil_method_green=dff_method["neuropil_method_green"],
                        regress_g_from_r=dff_method["regress_g_from_r"],
                        regress_r_from_g=dff_method["regress_r_from_g"],
                        baseline_method=dff_method["baseline_method"],
                        neu_coef=dff_method["neu_coef"],
                        keep_teleports=dff_method["keep_teleports"],
                    )

                # add deltaF/F timeseries to session object
                sess.add_timeseries(dff=dFF, dff2=dFF2)
                sess.add_pos_binned_trial_matrix(["dff", "dff2"], "pos")

            else:
                if calc_spks:
                    dFF, events = pp.dff(
                        sess.timeseries["F"],
                        sess.trial_start_inds,
                        sess.teleport_inds,
                        f_neu=sess.timeseries["Fneu"],
                        neuropil_method="subtract",
                        baseline_method=dff_method["baseline_method"],
                        subtract_baseline=True,
                        regress_ts=None,
                        neu_coef=dff_method["neu_coef"],
                        tau=sess.s2p_ops["tau"],
                        frame_rate=sess.scan_info["frame_rate"],
                        n_planes=sess.n_planes,
                        deconvolve=True,
                        keep_teleports=dff_method["keep_teleports"],
                    )
                    sess.add_timeseries(**{"events": events})
                    sess.add_pos_binned_trial_matrix("events", "pos")
                else:
                    dFF = pp.dff(
                        sess.timeseries["F"],
                        sess.trial_start_inds,
                        sess.teleport_inds,
                        f_neu=sess.timeseries["Fneu"],
                        neuropil_method="subtract",
                        baseline_method=dff_method["baseline_method"],
                        subtract_baseline=True,
                        regress_ts=None,
                        neu_coef=dff_method["neu_coef"],
                        keep_teleports=dff_method["keep_teleports"],
                    )

                sess.add_timeseries(dff=dFF)
                sess.add_pos_binned_trial_matrix("dff", "pos")
            sess.add_pos_binned_trial_matrix("spks", "pos")
            sess.add_timeseries(
                spks_norm=sess.timeseries["spks"]
                          / np.nanpercentile(sess.timeseries["spks"], 99, axis=1, keepdims=True)
            )
            sess.add_pos_binned_trial_matrix("spks_norm", "pos")

        isreward, morph, dream = behav.get_trial_types(sess)
        reward_zone, rz_label = behav.get_reward_zones(sess)

        trial_dict = behav.define_trial_subsets(sess, force_two_sets=trial_subsets)

        if calc_pcs:
            # Place cell calc: output boolean mask
            # mask: boolean mask of whether cell is a place cell
            # SI: spatial information
            # p: p value from permutation
            # perms: shuffle per cell

            if speed_thr > 0:
                speed = np.copy(sess.vr_data["speed"]._values)
            else:
                speed = None

            pc_out = calc_place_cells(
                sess,
                ts_key=ts_key,
                trial_subsets=trial_subsets,
                nperms=nperms,
                output_shuffle=output_shuffle,
                speed=speed,
                speed_thr=speed_thr,
                p_thr=p_thr,
                stability_thr=stability_thr,
                **pc_kwargs
            )
        else:
            # Initialize defaults for place cell logical, spatial info, p-value
            pc_out = {
                "masks0": [],
                "SI0": [],
                "p0": [],
                "perms0": [],
                "SI perms0": [],
                "masks1": [],
                "SI1": [],
                "p1": [],
                "perms1": [],
                "SI perms1": [],
            }

        all_anim[animal] = {
            "animal": animal,
            "sess": sess,
            "pc masks set0": pc_out["masks0"],
            "pc masks set1": pc_out["masks1"],
            "SI set0": pc_out["SI0"],
            "SI set1": pc_out["SI1"],
            "p set0": pc_out["p0"],
            "p set1": pc_out["p1"],
            "pc perms set0": pc_out["perms0"],
            "pc perms set1": pc_out["perms1"],
            "SI perms set0": pc_out["SI perms0"],
            "SI perms set1": pc_out["SI perms1"],
            "isreward": isreward,
            "morph": morph,
            "dream": dream,
            "rzone": reward_zone,
            "rz label": rz_label,
            "trial dict": trial_dict,
        }

        if stability_thr is not None:
            all_anim[animal].update({
                "stability p0": pc_out["stability p0"],
                "stability p1": pc_out["stability p1"],
                "stability masks0": pc_out["stability masks0"],
                "stability masks1": pc_out["stability masks1"],
            })

    return all_anim




