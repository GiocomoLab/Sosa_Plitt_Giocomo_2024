import numpy as np
import scipy as sp
# import scipy.io as spio
# import sqlite3 as sql
import pandas as pd
from datetime import datetime
from glob import glob
import os.path
import warnings

from sklearn.linear_model import LinearRegression as linreg
import TwoPUtils
import TwoPUtils.utilities as u
from TwoPUtils.utilities import nansmooth
from . import utilities as ut

# from suite2p.io.binary import BinaryFile
from suite2p.extraction import dcnv
# from PIL import Image


def create_sess(basedir, scandir, vrdir, animal, date, scene, session, scan_number,
                load_scaninfo=True,
                load_VR=True,
                load_suite2p=False,
                load_behavior=True,
                VR_only=False,
                nplanes=1,
                scanner="NLW",
                **trial_matrix_kwargs):
    """
    Wrapper to create sess class using TwoPUtils and add behavior data

    :param basedir: base path for preprocessed data, i.e. path_dict['preprocessed_root']
    :param scandir: path for sbx files, i.e. path_dict['sbx_root']
    :param vrdir: path for VR files, i.e. path_dict['VR_Data']
    :param animal: mouse name
    :param date: date of session, in mm_dd_yy format (because ScanBox)
    :param scene: VR scene name
    :param session: session number from sbx file name (i.e. 4 for '004_002')
    :param scan_number: scan number from sbx file name (i.e. 2 for '004_002')
    :param load_scaninfo: whether to add scanbox data
    :param load_VR: whether to add VR data and align VR to 2P
    :param load_suite2p: whether to add suite2p curation
    :param load_behavior: whether to add licking and speed data
    :return: completed sess class
    """

    if not VR_only:
        if scanner == "NLW":
            fullpath = os.path.join(
                basedir, date, scene, "%s_%03d_%03d" % (scene, session, scan_number))
            scanpath = os.path.join(
                scandir, date, scene, "%s_%03d_%03d" % (scene, session, scan_number))
            if os.path.exists(scanpath + ".sbx"):
                scan_header, scan_file = scanpath+'.mat', scanpath+'.sbx'
            else:
                scan_header, scan_file = fullpath+'.mat', fullpath+'.sbx'

            scan_info = TwoPUtils.scanner_tools.sbx_utils.loadmat(scan_header)
            n_planes = scan_info['n_planes']  # len(glob(planedir))
            print(f"Found {n_planes} planes from scan info")

        elif scanner == "ThorLabs":
            fullpath = os.path.join(
                basedir, date, "%s_%03d_%03d" % (scene, session, scan_number))
            scanpath = os.path.join(
                basedir, date, "%s_%03d_%03d" % (scene, session, scan_number))

            scan_file = glob(os.path.join(fullpath, 'Image_scan*.tif'))[0]
            scan_header = os.path.join(fullpath, "Experiment.xml")

            # find number of planes from suite2p dir
            # all scans have at least plane 0
            planedir = os.path.join(fullpath, "suite2p", "plane*")
            n_planes = len(glob(planedir))  # int(thor_metadata.zplanes)
            print(f"Found {nplanes} planes from suite2p dir")

        if n_planes != 0:
            nplanes = n_planes
        else:
            print(f"Overwriting with kwarg nplanes = {nplanes}")

    #         nplanes = np.maximum(scan_info['otwave'].shape[0],1) # this isn't always accurate

        s2p_path = os.path.join(fullpath, 'suite2p')

    else:
        scan_file, scan_header, s2p_path = None, None, None
        load_scaninfo = False
        load_VR = True
        load_suite2p = False
        load_behavior = True
        nplanes = 1
        VR_only = True

    vr_file = os.path.join(vrdir, '%s/%s/%s_%d.sqlite' %
                           (animal, date, scene, session))

    sess = TwoPUtils.sess.Session(**{
        'mouse': animal,
        'date': date,
        'scene': scene,
        'session': session,
        'scan_number': scan_number,
        'VR_only': VR_only,
        'basedir': fullpath,
        'prompt_for_keys': False,
        'scanner': scanner,
        'scan_file': scan_file,
        'scanheader_file': scan_header,
        's2p_path': s2p_path,
        'vr_filename': vr_file,
        'n_planes': nplanes,
    })

    append_session_data(sess,
                        scaninfo=load_scaninfo,
                        VR=load_VR,
                        suite2p=load_suite2p,
                        behavior=load_behavior,
                        **trial_matrix_kwargs)

    return sess


def append_session_data(sess, scaninfo=False, VR=False, suite2p=False, behavior=False,
                        **trial_matrix_kwargs):
    """
    complete the session class with relevant data
    - assumes the session class has already been created by TwoPUtils

    assign datatypes as True to append

    :param sess:
    :param scaninfo:
    :param VR:
    :param suite2p:
    :param behavior:
    :return:
    """

    if scaninfo:
        sess.load_scan_info()
    if VR:
        sess.align_VR_to_2P()
    print(sess.vr_data.shape)

    if suite2p:
        sess.load_suite2p_data()

    if behavior:
        if suite2p:
            sess.add_timeseries(licks=sess.vr_data['lick'],
                                rewards=sess.vr_data['reward'],
                                speed=sess.vr_data['speed'])
            sess.add_pos_binned_trial_matrix(
                ['speed'], 'pos', impute_nans=False, **trial_matrix_kwargs)
        else:
            # hack for now since we currently can't get speed without imaging frame times
            sess.add_timeseries(licks=sess.vr_data['lick'],
                                rewards=sess.vr_data['reward'])
        # add behavior trial matrices
        sess.add_pos_binned_trial_matrix(
            ['licks', 'rewards'], 'pos', impute_nans=False, **trial_matrix_kwargs)

    return sess


def vr_align_to_mock_2P(vr_dataframe, scan_info, run_ttl_check=False, n_planes=1):
    """
    For use when no imaging session accompanies the VR data.
    Basically downsamples the VR data to the 2P framerate.

    For aligning to actual 2P data, use TwoPUtils.preprocessing.vr_align_to_2P
    """

    fr = scan_info['frame_rate']  # frame rate
    lr = fr * scan_info['config']['lines'] / \
        scan_info['fov_repeats']  # line rate

    ttl_times = frames / fr + lines / lr

    numVRFrames = frames.shape[0]
    # print('numVRFrames', numVRFrames)

    # create empty pandas dataframe to store calcium aligned data
    ca_df = pd.DataFrame(columns=vr_dataframe.columns,
                         index=np.arange(int(scan_info['max_idx']/n_planes)))
    # ca_time = np.arange(0, 1 / fr * scan_info['max_idx'], 1 / fr)  # time on this even grid
    ca_time = np.arange(0, 1/fr * scan_info['max_idx'], n_planes/fr)
    ca_time[ca_time > ttl_times[-1]] = ttl_times[-1]
    print(f"{ttl_times.shape} ttl times,{ca_time.shape} ca2+ frame times")
    print(f"last time: VR {ttl_times[-1]}, ca2+ {ca_time[-1]}")
    # occasionally a 1 frame correction due to
    if (ca_time.shape[0] - ca_df.shape[0]) == 1:
        # scan stopping mid frame
        warnings.warn('one frame correction')
        ca_time = ca_time[:-1]

    ca_df.loc[:, 'time'] = ca_time
    # mask for when ttls have started on imaging clock
    mask = ca_time >= ttl_times[0]
    # (i.e. imaging started and stabilized, ~10s)

    # take VR frames for which there are valid TTLs
    vr_dataframe = vr_dataframe.iloc[-numVRFrames:]

    # find columns that exist in sqlite file from iterable
    def column_filter(columns): return [
        col for col in vr_dataframe.columns if col in columns]
    # linear interpolation of position and catmull rom spline "time" parameter
    lin_interp_cols = column_filter(('pos', 'posx', 'posy', 't'))

    f_mean = sp.interpolate.interp1d(
        ttl_times, vr_dataframe[lin_interp_cols]._values, axis=0, kind='slinear')
    ca_df.loc[mask, lin_interp_cols] = f_mean(ca_time[mask])
    ca_df.loc[~mask, 'pos'] = -500.

    # nearest frame interpolation
    near_interp_cols = column_filter(('morph', 'towerJitter', 'wallJitter',
                                      'bckgndJitter', 'trialnum', 'cmd', 'scanning', 'dreamland', 'LR'))

    f_nearest = sp.interpolate.interp1d(
        ttl_times, vr_dataframe[near_interp_cols]._values, axis=0, kind='nearest')
    ca_df.loc[mask, near_interp_cols] = f_nearest(ca_time[mask])
    ca_df.fillna(method='ffill', inplace=True)
    ca_df.loc[~mask, near_interp_cols] = -1.

    # integrate, interpolate and then take difference, to make sure data is not lost
    cumsum_interp_cols = column_filter(
        ('dz', 'lick', 'reward', 'tstart', 'teleport', 'rzone'))
    f_cumsum = sp.interpolate.interp1d(ttl_times, np.cumsum(vr_dataframe[cumsum_interp_cols]._values, axis=0), axis=0,
                                       kind='slinear')
    ca_cumsum = np.round(np.insert(f_cumsum(ca_time[mask]), 0, [
                         0]*len(cumsum_interp_cols), axis=0))
    if ca_cumsum[-1, -2] < ca_cumsum[-1, -3]:
        ca_cumsum[-1, -2] += 1

    ca_df.loc[mask, cumsum_interp_cols] = np.diff(ca_cumsum, axis=0)
    ca_df.loc[~mask, cumsum_interp_cols] = 0.

    # fill na here
    ca_df.loc[np.isnan(ca_df['teleport']._values), 'teleport'] = 0
    ca_df.loc[np.isnan(ca_df['tstart']._values), 'tstart'] = 0
    # if first tstart gets clipped
    if ca_df['teleport'].sum(axis=0) != ca_df['tstart'].sum(axis=0):
        warnings.warn("Number of teleports and trial starts don't match")
        if ca_df['teleport'].sum(axis=0) - ca_df['tstart'].sum(axis=0) == 1:
            warnings.warn(
                ("One more teleport and than trial start, Assuming the first trial start got clipped during "))
            ca_df['tstart'].iloc[0] = 1

        if ca_df['teleport'].sum(axis=0) - ca_df['tstart'].sum(axis=0) == -1:
            warnings.warn(
                ('One more trial start than teleport, assuming the final teleport got chopped'))
            ca_df['teleport'].iloc[-1] = 1
    # smooth instantaneous speed

    cum_dz = sp.ndimage.filters.gaussian_filter1d(
        np.cumsum(ca_df['dz']._values), 5)
    ca_df['dz'] = np.ediff1d(cum_dz, to_end=0)

    # ca_df['speed'].interpolate(method='linear', inplace=True)
    ca_df['speed'] = np.array(
        np.divide(ca_df['dz'], np.ediff1d(ca_df['time'], to_begin=1. / fr)))
    ca_df['speed'].iloc[0] = 0

    # calculate and smooth lick rate
    ca_df['lick rate'] = np.array(
        np.divide(ca_df['lick'], np.ediff1d(ca_df['time'], to_begin=1. / fr)))
    ca_df['lick rate'] = sp.ndimage.filters.gaussian_filter1d(
        ca_df['lick rate']._values, 5)

    # replace nans with 0s
    ca_df.fillna(value=0, inplace=True)
    return ca_df



def dff(f,  
        trial_starts, 
        teleports, 
        f_neu=None, 
        regress_ts=None, 
        neuropil_method=None, 
        bleedthrough_ts=None,
        neu_bleedthrough_ts=None,
        baseline_method='maximin', 
        subtract_baseline=True, 
        scrub_ts=None, 
        scrub_th=5,
        neu_coef=0.7, 
        tau=0.7, 
        frame_rate=15, 
        n_planes=1, 
        deconvolve=False, 
        keep_teleports=False):
    """
    Calculate dFF for 1 channel

    :param f: ROI fluorescence timeseries
    :param trial_starts:
    :param teleports:
    :param f_neu: neuropil fluorescence timeseries
    :param regress_ts:
    :param neuropil_method:
    :param bleedthrough_ts: timeseries of 2nd imaging color
    :param neu_bleedthrough_ts: timeseries of 2nd imaging color's neuropil
    :param baseline_method:
    :param subtract_baseline: True gives ∆F/F, False just normalizes F to the basline (divides only)
    :param scrub_ts:
    :param scrub_th:
    :param neu_coef:
    :param deconvolve: whether to deconvolve the dff trace into "events" (psuedo spikes)
    :return: dff and spks if deconvolve is True, otherwise just dff
    """

    print('baseline method =', baseline_method)

    # Check frame rate if deconvolving
    if deconvolve:
        if (frame_rate/n_planes) > 16:
            warnings.warn(
                f"Using frame rate per plane: {(frame_rate/n_planes)} -- is this correct?")

    f_ = np.zeros(f.shape) * np.nan

    if f_neu is not None:
        f_neu_ = np.zeros(f_neu.shape) * np.nan

    if keep_teleports:
        # if keeping the teleports, keep all the fluorescence from the start of imaging
        # EXCEPT the teleport sample itself, as this position values might be interpolated between
        # the end of the track and -50 (where the teleport jitter starts)
        print("keeping teleports")
        start_inds = np.append(
            trial_starts[0],
            np.array(teleports[:-1] + 2)
        ).tolist()
        # ^ add 2 so with start-1 we will start 1 sample after the teleport
        stop_inds = teleports.to_list()
        # we will subtract 1 off this index below
    else:
        # if not keeping the teleports, keep only the fluorescence on each trial
        start_inds = trial_starts.tolist()
        stop_inds = teleports.tolist()

    for i, (start, stop) in enumerate(zip(start_inds, stop_inds)):
        f_[:, start - 1:stop - 1] = f[:, start - 1:stop - 1]

        if f_neu is not None:
            f_neu_[:, start - 1:stop - 1] = f_neu[:, start - 1:stop - 1]

    # green channel bleedthrough correction: regress green channel from red channel
    # For each cell, predict red from green channel, subtract prediction to get residual, and add back in intercept;
    # So red signal will be residual+intercept
    nanmask = ~np.isnan(f_[0, :])

    if bleedthrough_ts is not None:
        # subtract the mean so only the time-varying component is regressed out
        bleedthrough_ts = np.copy(bleedthrough_ts)
        bleedthrough_ts[:, nanmask] = bleedthrough_ts[:, nanmask] - \
            np.nanmean(bleedthrough_ts[:, nanmask], axis=1, keepdims=True)
        for cell in range(f_.shape[0]):
            # linear regression from scikitlearn
            lr = linreg().fit(
                bleedthrough_ts[cell:cell + 1, nanmask].T, f_[cell, nanmask])
            f_[cell, nanmask] = f_[cell, nanmask] - \
                lr.predict(
                    bleedthrough_ts[cell:cell + 1, nanmask].T) + lr.intercept_

    if regress_ts is not None:
        print(regress_ts.shape, f_.shape)
        regress_ts = np.copy(regress_ts)
        lr = linreg().fit(regress_ts[:, nanmask].T, f_[:, nanmask].T)
        f_[:, nanmask] = f_[:, nanmask] - 1. * \
            lr.predict(regress_ts[:, nanmask].T).T + \
            lr.intercept_[:, np.newaxis]

    if scrub_ts is not None:
        scrub_mask = scrub_ts >= scrub_th
        f_[:, scrub_mask] = np.nan

    if f_neu is not None and neu_bleedthrough_ts is not None:
        # subtract the mean so only the time-varying component is regressed out
        neu_bleedthrough_ts = np.copy(neu_bleedthrough_ts)
        neu_bleedthrough_ts[:, nanmask] = neu_bleedthrough_ts[:, nanmask] - \
            np.nanmean(neu_bleedthrough_ts[:, nanmask], axis=1, keepdims=True)
        for cell in range(f_neu_.shape[0]):
            lr = linreg().fit(
                neu_bleedthrough_ts[cell:cell + 1, nanmask].T, f_neu_[cell, nanmask])
            f_neu_[cell, nanmask] = f_neu_[cell, nanmask] - neu_coef * \
                lr.predict(
                    neu_bleedthrough_ts[cell:cell + 1, nanmask].T) + lr.intercept_

    if neuropil_method == 'subtract' and bleedthrough_ts is not None and neu_bleedthrough_ts is None:
        print(
            'Regressing ROI bleedthrough but not neuropil bleedthrough -- is this correct?')

    # once bleedthrough is subtracted, do neuropil correction

    if neuropil_method == 'subtract':
        f_ -= neu_coef * f_neu_
    elif neuropil_method == 'regress':
        raise NotImplementedError
    elif neuropil_method is None:
        pass
    else:
        print('Undefined neuropil_method')
        raise NotImplementedError

    print('No minimum subtraction')

    # Caluclate baseline for chan 1
    flow = np.zeros(f_.shape) * np.nan
    spks = np.zeros(f_.shape) * np.nan

    # get baseline within each trial
    for i, (start, stop) in enumerate(zip(start_inds, stop_inds)):

        # if we subtracted neuropil, add back in the neuropil mean in each trial
        # such that dff values will be close to true ∆F/F, and we don't divide by small numbers
        if neuropil_method == 'subtract':
            f_[:, start-1:stop-1] = f_[:, start-1:stop-1] + neu_coef * np.nanmean(
                f_neu_[:, start-1:stop-1], axis=1, keepdims=True)

        if baseline_method == 'maximin':
            flow[:, start - 1:stop -
                 1] = ut.nansmooth(f_[:, start-1:stop-1], [0, 15])
            # [0., 30])  # cut out ITIs and smooth signal
            # minimum filter, taking min val over 20 sec
            flow[:, start-1:stop-1] = sp.ndimage.filters.minimum_filter1d(
                flow[:, start-1:stop-1], int(300), axis=-1)
            flow[:, start-1:stop-1] = sp.ndimage.filters.maximum_filter1d(
                flow[:, start-1:stop-1], int(300), axis=-1)  # max filter with same window (dilation)
        elif baseline_method == 'maxsmooth':
            flow[:, start-1:stop -
                 1] = ut.nansmooth(f_[:, start-1:stop-1], [0, stop-start])
        else:
            print('Undefined baseline_method')
            raise NotImplementedError

    # to get deltaF/F: subtract baseline from initial signal, divide by abs(baseline)
    # baseline can some times end up as negative due to regression
    dff = np.zeros(f_.shape) * np.nan
    if subtract_baseline:
        dff[:, nanmask] = (f_[:, nanmask] - flow[:, nanmask]
                           ) / np.abs(flow[:, nanmask])
    else:
        print('divide baseline only')
        dff[:, nanmask] = (f_[:, nanmask]) / (np.abs(flow[:, nanmask])) - 1

    print('dff:', dff.shape, 'min ', np.nanmin(dff), 'max ', np.nanmax(dff))

    # Smooth the deltaF/F transients by 2 time bins
    for i, (start, stop) in enumerate(zip(start_inds, stop_inds)):
        dff[:, start - 1:stop -
            1] = ut.nansmooth(dff[:, start - 1:stop - 1], 2, axis=1)
        # After smoothing: Get "spikes" with calcium kernel deconvolution from suite2p
        if deconvolve:
            spks[:, start-1:stop-1] = dcnv.oasis(dff[:, start-1:stop-1], 2000, tau,
                                                 (frame_rate/n_planes)
                                                 )

    if deconvolve:
        return dff, spks
    else:
        return dff



