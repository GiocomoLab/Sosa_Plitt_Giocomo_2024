import numpy as np
from scipy.optimize import curve_fit 
from scipy import stats

import uncertainties.unumpy as unp
import uncertainties as unc


def linear_reg(x,y,print_stats=False):
    """
    Return linear regression with confidence interval

    :param x:
    :param y:
    :param print_stats: whether to display stats
    :return: line: x, y values and std (confidence interval) of the best-fit line;
            params: slope and intercept (Â± std) of the linear fit,
                    r-squared, and p-value of the regression
    """

    f = lambda x,a,b: np.multiply(a,x) + b
    
    popt, pcov = curve_fit(f, x, y)

    # retrieve parameter values
    a = popt[0]
    b = popt[1]

    # compute r^2
    r2 = 1.0-(sum((y-f(x,a,b))**2)/((len(y)-1.0)*np.var(y,ddof=1)))
    if print_stats:
        print('Optimal Values')
        print('slope: ' + str(a))
        print('intercept: ' + str(b))
        print('R^2: ' + str(r2))

    # calculate parameter confidence interval
    aa,bb = unc.correlated_values(popt, pcov)
    if print_stats:
        print('Uncertainty')
        print('slope: ' + str(aa))
        print('intercept: ' + str(bb))

    # calculate line and regression confidence interval
    x_sort = np.sort(x,kind='stable')
    px = np.linspace(x_sort[0], x_sort[-1], 100)
    py = aa*px+bb
    nom = unp.nominal_values(py)
    std = unp.std_devs(py)
    
    line = {'x': px,
            'y': nom,
            'std': std}
    
    lr_stats = stats.linregress(x,y=y)
    params = {'slope': aa,
              'intercept': bb,
              'r2': r2,
              'r': lr_stats.rvalue,
              'p': lr_stats.pvalue}
    if print_stats:
        print('r: ' + str(lr_stats.rvalue))
        print('p: ' + str(lr_stats.pvalue))
    
    return a,b,line,params


def logarithmic_reg(x,y,print_stats=False):

    """
    Return logarithmic regression with confidence interval

    :param x:
    :param y:
    :param print_stats: whether to display stats
    :return: curve: x, y values and std (confidence interval) of the best-fit curve;
            params: coefficient (a) and intercept (b) of the fit curve,
                    mean-squared-error (mse) of the fit
    """

    f = lambda x,a,b: a*np.log(x+1) + b
    # x values cannot be 0 for log - add 1 to get rid of 0 indexing

    popt, pcov = curve_fit(f, x, y)

    # retrieve parameter values
    a = popt[0]
    b = popt[1]

    # r2 not valid for nonlinear regression; so we do mean squared error instead
    mse = sum((y - f(x,a,b))**2) / len(y)

    # calculate parameter confidence interval
    aa,bb = unc.correlated_values(popt, pcov)
    if print_stats:
        print('Uncertainty')
        print('slope: ' + str(aa))
        print('intercept: ' + str(bb))
        print('MSE: ' + str(mse))

    # calculate curve and regression confidence interval
    x_sort = np.sort(x,kind='stable')
    px = np.linspace(x_sort[0], x_sort[-1], 100)
    py = f(px,aa,bb) #aa*np.log(px+1) + bb
    nom = unp.nominal_values(py)
    std = unp.std_devs(py)

    curve = {'x': px,
            'y': nom,
            'std': std}

    params = {'a': a,
              'b': b,
              'MSE': mse,
              }
    
    return curve, params


def exponential_reg(x,y,initial_guess=None,print_stats=False):

    """
    Return exponential regression with confidence interval

    :param x:
    :param y:
    :param print_stats: whether to display stats
    :return: curve: x, y values and std (confidence interval) of the best-fit curve;
            params: coefficient (a) and intercept (b) of the fit curve,
                    mean-squared-error (mse) of the fit
    """

    f = lambda x,a,b: a * np.exp(-1*(x+1)) + b
    # x values cannot be 0 for log - add 1 to get rid of 0 indexing

    popt, pcov = curve_fit(f, x, y, p0 = initial_guess)

    # retrieve parameter values
    a = popt[0]
    b = popt[1]
    # c = popt[2]

    # r2 not valid for nonlinear regression; so we do mean squared error instead
    mse = sum((y - f(x,a,b))**2) / len(y)

    # calculate parameter confidence interval
    aa,bb = unc.correlated_values(popt, pcov)
    if print_stats:
        print('Uncertainty')
        print('slope: ' + str(aa))
        print('intercept: ' + str(bb))
        print('MSE: ' + str(mse))

    # calculate curve and regression confidence interval
    x_sort = np.sort(x,kind='stable')
    px = np.linspace(x_sort[0], x_sort[-1], 100)
    py = f(px,aa,bb) #aa*np.log(px+1) + bb
    nom = unp.nominal_values(py)
    std = unp.std_devs(py)

    curve = {'x': px,
            'y': nom,
            'std': std}

    params = {'a': a,
              'b': b,
              # 'c': c,
              'MSE': mse,
              }
    
    return curve, params


def sigmoid(x, L ,x0, k, b):
    """ 
    Create a sigmoid with some input parameters
    """
    y = L / (1 + np.exp(-k*(x-x0))) + b
    return (y)

def fit_sigmoid(ydata):
    
    """
    Fit a sigmoid to the data and find the inflection point
    """
    xdata = np.arange(0, len(ydata))
    # mandatory initial guess for sigmoid params L, x0, k, b
    p0 = [max(ydata), np.median(xdata),1,min(ydata)]

    try:
        popt, _ = curve_fit(sigmoid, xdata, ydata,p0, method='dogbox')
        sigmoid_curve = sigmoid(xdata, popt[0], popt[1], popt[2], popt[3])
        # find the inflection point or midpoint between clusters (where the sigmoid plateaus):
        sigmoid_gradient = np.gradient(np.gradient(sigmoid_curve))
        # find where the sign of the sigmoid second derivative switches from +1 to -1 (diff== -2)
        inflection = np.where(np.ediff1d(np.sign(sigmoid_gradient),to_begin=0)==-2)[0]
    except:
        sigmoid_curve = np.nan
        inflection = np.nan
    
    return sigmoid_curve, inflection