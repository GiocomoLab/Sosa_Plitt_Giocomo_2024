import math
import sys
import os
import numpy as np
import scipy as sp
import pandas as pd
import warnings
from tqdm import tqdm
import copy
from astropy import stats

from reward_relative import behavior as behav
from reward_relative import utilities as ut
from reward_relative import spatial
from reward_relative import dayData as dd
from reward_relative import circ
from reward_relative import rewardAnalysis as ra


import sklearn
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.preprocessing import SplineTransformer, QuantileTransformer
import tensorflow as tf
import keras
import sys
sys.path.append("/home/mari/local_repos/2p_repos/GLM_Tensorflow_2/code")
import glm_class as glm

## Several of the modules below are adaptations of functions originally
## written by Shih-Yi Tseng, https://github.com/sytseng/GLM_Tensorflow_2/tree/main


def create_design_matrix(animal,
                         sess,
                         pos_predictors = [
                                       'rewarded',
                                       'omission',
                                          ],
                         mvt_predictors = ['speed',
                                           'accel',
                                           'licks',
                                          ],
                         n_pos_bases=45,
                         start_pos=0,
                         end_pos=450,
                         lick_smooth_sigma = 2,
                         movt_spline = 'bspline', # or 'cosine'
                         rel_pos_type = 'circular', # or 'linear'
                         use_speed_thr = 2,
                        ):
    
    """
    Create design matrix X for Mari's calcium imaging data
    
    To-do: finish docstring
    """
    
    orig_pos = np.copy(sess.vr_data['pos']._values)
    
    deconv_activity = np.zeros(sess.timeseries['events'].shape)*np.nan
    rel_pos = np.zeros(orig_pos.shape)*np.nan #linear distance from reward
    pos = np.zeros(orig_pos.shape)*np.nan
    trial_ids = np.zeros(orig_pos.shape)*np.nan
    was_reward = np.zeros(orig_pos.shape)*np.nan # set to 1 for samples following reward within a trial
    was_omission = np.zeros(orig_pos.shape)*np.nan
    licks = np.squeeze(np.copy(sess.timeseries['licks']))

    # reward zone entry index on omission trials
    rz_ind_on_omiss = ra.get_omission_inds(sess)
    omission = ra.get_omission_trials(sess)
    reward_zone, _ = behav.get_reward_zones(sess)

    reward_inds = np.where(sess.timeseries['rewards'][0])[0]

    trial_starts = sess.trial_start_inds
    teleports = sess.teleport_inds

    for i, (start,stop) in enumerate(zip(trial_starts.tolist(),teleports.tolist())):

        
        rzone_this_trial = reward_zone[i,0]
        if rel_pos_type == 'linear':
            ## set the reward location as the trajectory start without making it circular
            aligned_pos = orig_pos[start-1:stop-1] - rzone_this_trial
            aligned_pos[aligned_pos<0] = aligned_pos[aligned_pos<0] + end_pos
            rel_pos[start-1:stop-1] = aligned_pos
        elif rel_pos_type == 'circular':
            
            circ_rzone_this_trial = spatial.pos_cm_to_rad(rzone_this_trial, end_pos,start_pos)

            rel_pos[start-1:stop-1] = spatial.pos_cm_to_rad(orig_pos[start-1:stop-1], end_pos, start_pos)
            rel_pos[start-1:stop-1] = circ.wrap(rel_pos[start-1:stop-1] - circ_rzone_this_trial)

        # original position to keep
        pos[start-1:stop-1] = orig_pos[start-1:stop-1]
        # activity to keep
        deconv_activity[:, start-1:stop-1] = sess.timeseries['events'][:, start-1:stop-1]
        
        #trial IDs (repeated for every sample)
        trial_ids[start-1:stop-1] = np.ones((len(trial_ids[start-1:stop-1]),))*i

        # find the reward in this trial
        find_reward = np.where(np.logical_and(reward_inds>start-1, reward_inds<stop-1))[0]
        if len(find_reward) > 0:
            was_reward[reward_inds[find_reward[0]]:stop-1] = 1
            was_reward[start-1:reward_inds[find_reward[0]]] = 0
        else:
            was_reward[start-1:stop-1] = 0

        if i in omission['trials']:
            # rzone entry index on omission trials
            was_omission[rz_ind_on_omiss[omission['trials'].index(i)] : stop-1] = 1
            was_omission[start-1:rz_ind_on_omiss[omission['trials'].index(i)]] = 0
        else:
            was_omission[start-1:stop-1] = 0
            
        # find licks with sensor error correction
        # if >50% of samples have a cumulative lick count of >2
        if sum(licks[start:stop] > 2)/len(licks[start-1:stop-1]) > 0.35:
            licks[start:stop] = np.nan
            
    # Get rid of additional nans
    findnans = np.isnan(sess.timeseries['events'][0,:])
    
    # licks = ut.nansmooth(licks,2)
    licks[licks > 1] = 1
    licks[findnans] = np.nan

    tmp_speed = np.squeeze(np.copy(sess.timeseries['speed'][0]))
    tmp_accel = np.ediff1d(tmp_speed, to_end=0)
    tmp_speed[findnans] = np.nan
    if use_speed_thr is not None:
        tmp_speed[tmp_speed<use_speed_thr] = np.nan

    # mask_out_nans = ~np.isnan(tmp_speed) #~np.isnan(sess.timeseries['events'][0,:])
    mask_out_nans = ((~np.isnan(tmp_speed)) & (~np.isnan(licks)))

    # initialized dataframe for design matrix (= X)
    data = pd.DataFrame()
    data['speed'] = tmp_speed[mask_out_nans]
    data['accel'] = tmp_accel[mask_out_nans]
    data['licks'] = ut.nansmooth(licks[mask_out_nans],2)
    data['trials'] = trial_ids[mask_out_nans] # don't actually include in ultimate design matrix, just for splitting train/test
    data['omission'] = was_omission[mask_out_nans] - 0.5 # zero-center it
    data['rewarded'] = was_reward[mask_out_nans] - 0.5 # zero-center it
    data['pos'] = pos[mask_out_nans]
    data['rel_pos'] = rel_pos[mask_out_nans] 
    # currently distance from each reward zone start (counting up as the animal moves away from a given reward)

    # Deconvolved events (= Y)
    deconv = np.copy(deconv_activity[:,mask_out_nans])     
                        
    
    ## Create position basis functions
    print('Performing basis expansion...')

    # Linearly space the centers of the position bases
    pos_centers = np.linspace(start_pos, end_pos, n_pos_bases)

    # Set width of the position bases as 4 times spacing
    width_to_spacing_ratio = 4
    pos_width = width_to_spacing_ratio * sp.stats.mode(np.diff(pos_centers))[0][0]

    # Evaluate the values of the position series on each base
    # Note: the number of datapoints here is arbitrary; it just has to be of enough resolution when we visualize the basis functions
    positions = np.linspace(start_pos, end_pos, 500)
    pos_bases = create_cosine_bumps(positions, pos_centers, pos_width * np.ones_like(pos_centers))
    
    ## Apply position basis functions to forward maze position
    # Evaluate the basis expanded forward position at each basis
    posF = data['pos'].copy().values # extract time series of real forward position for data
    f_pos_bases = create_cosine_bumps(posF, pos_centers, pos_width * np.ones_like(pos_centers)) # position basis expansion
    f_pos_names = [f'fPos_bump{i}' for i in range(len(pos_centers))] # create a list of names for each expanded feature
    
    # ------ #
    # do the same for relative pos
    relpos = data['rel_pos'].copy().values
    # Set parameters
    start_relpos = -np.pi
    end_relpos = np.pi
    n_relpos_bases = n_pos_bases
    # Linearly space the centers of the position bases
    relpos_centers = np.linspace(start_relpos, end_relpos, n_relpos_bases)

    # Set width of the position bases as 4 times spacing
    width_to_spacing_ratio = 4
    relpos_width = width_to_spacing_ratio * sp.stats.mode(np.diff(relpos_centers))[0][0]

    # Same for relative positions
    relpositions = np.linspace(start_relpos, end_relpos, 500)
    rel_pos_bases = create_cosine_bumps(relpositions, relpos_centers, relpos_width * np.ones_like(relpos_centers))

    relposF = data['rel_pos'].copy().values
    f_relpos_bases = create_cosine_bumps(relposF, relpos_centers, 
                                         relpos_width * np.ones_like(relpos_centers))
    f_relpos_names = [f'relPos_bump{i}' for i in range(len(relpos_centers))]

    ## Apply forward position bases to task variables
    var_names = pos_predictors #[~np.isin(pos_predictors,['pos','rel_pos'])]
                                        #['licks','rewarded','omission']

    # Initialize features and names with expanded position bases
    expanded_features_pos = np.full((f_pos_bases.shape[0], n_pos_bases * (len(var_names) + 1)), np.NaN)
    expanded_features_pos[:, :f_pos_bases.shape[1]] = f_pos_bases.copy()
    expanded_feature_names_pos = [name for name in f_pos_names] #[f'trialPhase_{base_name}' for base_name in f_pos_names]

    # Multiply individual variables with expanded position predictors
    for i, var in enumerate(var_names):  
        expanded_features_pos[:, (i + 1) * n_pos_bases:(i + 2) * n_pos_bases] = data[var][:, None] * f_pos_bases
        expanded_feature_names_pos += [f'{var}_{base_name}' for base_name in f_pos_names]

    print('Shape of position expanded features =', expanded_features_pos.shape, 
          '\nNumber of position expanded features =', len(expanded_feature_names_pos))
    
    ## Create movement bases
    if movt_spline == 'bspline':
        expanded_features_mvt, expanded_feature_names_mvt, mvt_basis = create_movt_bspline(data,
                                                                                          mvt_predictors)
        
    elif movt_spline == 'cosine':
        expanded_features_mvt, expanded_feature_names_mvt, mvt_basis = create_movt_cosine(data,
                                                                                         mvt_predictors)
    else:
        raise NotImplementedError('Movement spline type not defined')
        
        
    # Combine all features and feature names
    # all_features = np.concatenate((expanded_features_pos, f_relpos_bases, f_speed_bases, f_accel_bases), axis=1)
    all_features = np.concatenate((expanded_features_pos, f_relpos_bases, expanded_features_mvt), axis=1)
    all_feature_names = expanded_feature_names_pos.copy()
    all_feature_names.extend(f_relpos_names)
    all_feature_names.extend(expanded_feature_names_mvt)

    print('Shape of all features combined =', all_features.shape, '\nNumber of all expanded features =', len(all_feature_names))
    # print(all_feature_names)
    
    # Parse feature group
    group_size, group_name, group_ind = parse_group_from_feature_names(all_feature_names)
    print('Number of groups =', len(group_size))
    print(group_size), print(group_name)

    # Clean up design matrix and z-score along sample dimension
    # we zscore the variables so they exert even effect on the model
    all_features[np.isnan(all_features)] = 0
    X = sp.stats.zscore(all_features, axis = 0)

    # Multiply deconvolved activity by 10 to mimic spike number
    # And transpose so Y is timepoints x neurons
    Y = 10. * deconv.T

    group_info = {'group_size': group_size,
                  'group_name': group_name,
                  'group_ind': group_ind
                 }
    
    return X, Y, data, group_info, pos_bases, pos_centers, mvt_basis


def create_movt_bspline(data, mvt_var_names, degree=3, n_knots=5):
    
    """
    Create B-spline basis functions for quantiles of speed and acceleration
    """
    
    # Create b-splines on a support between 0 to 1 (works on quantile transformed data; see next block below)
    mvt_bins = np.arange(0,1,0.02).reshape(-1,1)
    # initialize sklearn SplineTransformer
    spline = SplineTransformer(degree = 3, n_knots = 5, knots = 'uniform')
    mvt_bspl = spline.fit_transform(mvt_bins)

    ## Apply b-spline expansion to movement variables

    # Initialize for expanded features and names
    expanded_features_mvt = []
    expanded_feature_names_mvt = [] 

    # Specify b-spline setting
    n_bsplines = degree + n_knots - 1

    # Loop over variables in movement variables and create b-spline expansion and expanded feature names
    for i, var in enumerate(mvt_var_names):
        # quantile transform velocity
        this_var_quant = QuantileTransformer(n_quantiles=1000).fit_transform(data[var].values.reshape(-1,1))
        # transform velocity quantiles into b-splines
        these_splines = SplineTransformer(degree=degree, n_knots=n_knots, knots='uniform').fit_transform(this_var_quant)
        # append features and feature names
        expanded_features_mvt.append(these_splines)
        expanded_feature_names_mvt.extend([f'{var}_bump{i}' for i in range(n_bsplines)])

    # Concatenate expanded features for all movement variables
    expanded_features_mvt = np.hstack(expanded_features_mvt)

    print('Shape of expanded movement features =', expanded_features_mvt.shape, 
          '\nNumber of expanded movement features =', len(expanded_feature_names_mvt))
    
    return expanded_features_mvt, expanded_feature_names_mvt, mvt_bspl

    
def create_movt_cosine(data, n_speed_bases=20):
    
    """
    Create cosine bumps for movement variables
    """
    
    start_speed = data['speed'].min()
    end_speed = data['speed'].max()

    # Linearly space the centers of the position bases
    speed_centers = np.linspace(start_speed, end_speed, n_speed_bases)

    # Set width of the position bases as 4 times spacing
    width_to_spacing_ratio = 4
    speed_width = width_to_spacing_ratio * sp.stats.mode(np.diff(speed_centers))[0][0]

    # Evaluate the values of the position series on each base (for visualization purpose)
    # Note: the number of datapoints here is arbitrary; it just has to be of enough resolution when we visualize the basis functions
    speed_range = np.linspace(start_speed, end_speed, 500)
    speed_bases = create_cosine_bumps(speed_range, speed_centers, speed_width * np.ones_like(speed_centers))
    
    ## Apply speed basis functions to z-scored speed
    # Evaluate the basis expanded forward position at each basis
    spF = data['speed'].copy().values # extract time series of real forward position for data
    f_speed_bases = create_cosine_bumps(spF, speed_centers, speed_width * np.ones_like(speed_centers)) # position basis expansion
    speed_names = [f'speed_bump{i}' for i in range(len(speed_centers))] # create a list of names for each expanded feature
    
    # Acceleration
    ## Create accel basis functions
    # Set parameters
    start_accel = data['accel'].min()
    end_accel = data['accel'].max()
    n_accel_bases = 20

    # Linearly space the centers of the position bases
    accel_centers = np.linspace(start_accel, end_accel, n_accel_bases)

    # Set width of the position bases as 4 times spacing
    width_to_spacing_ratio = 4
    accel_width = width_to_spacing_ratio * sp.stats.mode(np.diff(accel_centers))[0][0]

    # Evaluate the values of the position series on each base (for visualization purpose)
    # Note: the number of datapoints here is arbitrary; it just has to be of enough resolution when we visualize the basis functions
    accel_range = np.linspace(start_accel, end_accel, 500)
    accel_bases = create_cosine_bumps(accel_range, accel_centers, accel_width * np.ones_like(accel_centers))
    
    ## Apply speed basis functions to z-scored speed
    # Evaluate the basis expanded forward position at each basis
    spF = data['accel'].copy().values # extract time series of real forward position for data
    f_accel_bases = create_cosine_bumps(spF, accel_centers, accel_width * np.ones_like(accel_centers)) # position basis expansion
    accel_names = [f'accel_bump{i}' for i in range(len(accel_centers))] # create a list of names for each expanded feature

#     expanded_features_mvt = []
#     expanded_feature_names_mvt = []
#     expanded_features_mvt.append(f_speed_bases)
#     expanded_features_mvt.append(f_accel_bases)
#     expanded_feature_names_mvt.extend(speed_names)
#     expanded_feature_names_mvt.extend(accel_names)
    
    # Apply b-spline expansion to movement variables
    mvt_var_names = ['speed', 'accel'] #list(data['movement_var'].keys())

    # Initialize for expanded features and names
    expanded_features_mvt = []
    expanded_feature_names_mvt = [] 

    # Loop over variables in movement variables and create expanded feature names
    # for i, var in enumerate(mvt_var_names):
    expanded_features_mvt.append(f_speed_bases)
    expanded_feature_names_mvt.extend([f'speed_bump{i}' for i in range(n_speed_bases)])
    expanded_features_mvt.append(f_accel_bases)
    expanded_feature_names_mvt.extend([f'accel_bump{i}' for i in range(n_accel_bases)])

    # Concatenate expanded features for all movement variables
    expanded_features_mvt = np.hstack(expanded_features_mvt)

    print('Shape of expanded movement features =', expanded_features_mvt.shape, 
          '\nNumber of expanded movement features =', len(expanded_feature_names_mvt))

    return expanded_features_mvt, expanded_feature_names_mvt, speed_bases
    
    
def split_data(X,Y,data,train_size=0.85,random_state=42):
    
    """
    Get indices for splitting according to trial_id 
    """
    
    # To-Do: changed data['trials'] to a trial_ids arg
    n_samples = X.shape[0]
    group_id = data['trials']
    gss = GroupShuffleSplit(n_splits = 5, train_size = train_size, random_state = random_state) #42)
    train_idx, test_idx = next(gss.split(X, Y, group_id))

    # Split data into train and test set
    X_train = X[train_idx,:]
    Y_train = Y[train_idx,:]
    X_test = X[test_idx,:]
    Y_test = Y[test_idx,:]
    trial_id_train = group_id[train_idx]
    
    return X_train, Y_train, X_test, Y_test, trial_id_train, train_idx

def split_data_by_trial_type(X,Y,data,
                             omission_idx,
                             not_omission_set0,
                             not_omission_set1,
                             train_size=0.85,random_state=42):
    
    """
    Split data into train and test sets, including trials of each type in
    both train and test
    """
    
    train_idx = []
    test_idx = []
    n_samples = X.shape[0]
    group_id = data['trials']
    # get omission trial indices
    gss = GroupShuffleSplit(n_splits = 5, train_size = train_size, random_state = random_state) #42)
    train_idx_omiss, test_idx_omiss = next(gss.split(X[omission_idx.index,:],
                                         Y[omission_idx.index,:], 
                                         group_id[omission_idx.index]))
    trial_id_train = group_id[omission_idx.index].iloc[train_idx]
    np.unique(trial_id_train)

    train_idx = np.asarray(omission_idx.index[train_idx_omiss])
    test_idx = np.asarray(omission_idx.index[test_idx_omiss])

    # get set0 rewarded indices
    gss = GroupShuffleSplit(n_splits = 5, train_size = train_size, random_state = random_state)
    train_idx_set0, test_idx_set0 = next(gss.split(X[not_omission_set0.index,:],
                                         Y[not_omission_set0.index,:], 
                                         group_id[not_omission_set0.index]))

    train_idx = np.append(train_idx,
                         np.asarray(not_omission_set0.index[train_idx_set0]))
    test_idx = np.append(test_idx, 
                         np.asarray(not_omission_set0.index[test_idx_set0]))
    # train_idx.append(not_omission_set0.index[train_idx_set0].tolist())
    # test_idx.append(not_omission_set0.index[test_idx_set0].tolist())

    # get set1 rewarded indices
    gss = GroupShuffleSplit(n_splits = 5, train_size = train_size, random_state = random_state)
    train_idx_set1, test_idx_set1 = next(gss.split(X[not_omission_set1.index,:],
                                         Y[not_omission_set1.index,:], 
                                         group_id[not_omission_set1.index]))

    # train_idx.append(not_omission_set1.index[train_idx_set1].tolist())
    # test_idx.append(not_omission_set1.index[test_idx_set1].tolist())
    train_idx = np.append(train_idx,
                         np.asarray(not_omission_set1.index[train_idx_set1]))
    test_idx = np.append(test_idx, 
                         np.asarray(not_omission_set1.index[test_idx_set1]))

    train_idx = np.sort(np.unique(train_idx))
    test_idx = np.sort(np.unique(test_idx))

    print(np.sum([train_idx.shape[0], test_idx.shape[0]]), X.shape[0])
    # Split data into train and test set
    X_train = X[train_idx,:]
    Y_train = Y[train_idx,:]
    X_test = X[test_idx,:]
    Y_test = Y[test_idx,:]
    trial_id_train = group_id[train_idx]

    return X_train, Y_train, X_test, Y_test, trial_id_train, train_idx


def create_cosine_bumps(x, centers, widths):
    '''Create raised cosine bumps

    Input parameters::
    x: x positions to evaluate the cosine bumps on, ndarray of shape (n_samples, )
    centers: contains center positions of bumps, ndarray of shape (number of bumps, )
    widths: the width of each bump, should be same shape as centers

    Returns::
    bases: basis functions, ndarray of shape (n_samples, number of bumps)
    '''
    # Sanity check
    assert centers.shape == widths.shape, 'Centers and widths should have same number of elements'  
    x_reshape = x.reshape(-1,)

    # Create empty array for basis functions
    bases = np.full((x.shape[0], centers.shape[0]), np.NaN)

    # Loop over center positions
    for idx, cent in enumerate(centers):
        bases[:, idx] = (np.cos(2 * np.pi * (x_reshape - cent) / widths[idx]) * 0.5 + 0.5) * \
                        (np.absolute(x_reshape - cent) < widths[idx] / 2)

    return bases


def parse_group_from_feature_names(feature_names):
    ''' 
    Parse feature_names into groups using hand-crafted rules

    Input parameters:: 
    feature_names: List of feature names. In this example, expanded features must contain bumpX in the name

    Returns:: 
    group_size: list of number of features in each group
    group_name: name of each group
    group_ind: group index of each feature in feature_names, ndarray of size (len(feature_names),)
    '''
    
    # Find expanded features and their number of sub-features:
    group_size = list()
    group_name = list()
    group_ind = list()
    for name in feature_names:
        if 'bump' not in name:
            # Non-bump expanded feature:
            group_size.append(1)
            group_name.append(name)

        elif 'bump0' in name:
            # First bump of a bump-expanded feature:
            group_size.append(1)
            group_name.append(name[:-6])

        else: 
            # Subsequent time shifts and bumps
            group_size[-1] += 1  

    # Create group index for each feature
    for i_group, this_size in enumerate(group_size):
        group_ind += [i_group]*this_size
    
    return group_size, group_name, np.array(group_ind)
    
    
def pos_binning(X, posF, pos_centers, pos_half_width):
    '''
    Bin input X by positions and time

    Input parameters::
    X: variable for binning, ndarray
    posF: position for each point in X, ndarray of shape (X.shape[0],)
    time_from_cho: time from choice point for each point in X, ndarray of shape (X.shape[0],)
    pos_centers: center locations for position bins, ndarray
    pos_half_width: half width of position center, float
    tm_centers: center locations for time bins, ndarray
    tm_half_width: half width of time bins, float

    Returns:
    X_pos: position-binned X, ndarray of shape (n_pos_bins, X.shape[1])
    X_tm: time-binned X, ndarray of shape (n_tm_bins, X.shape[1])
    '''
    # Sanity check and prelocate
    if X.ndim == 1:
        X = X.reshape(-1,1)
    X_pos = np.full((pos_centers.shape[0],X.shape[1]),np.NaN)
    # X_tm = np.full((tm_centers.shape[0],X.shape[1]),np.NaN)
    
    # Calcuate position-binned X
    for pos_ind, pos_cent in enumerate(pos_centers):
        these_frames = np.logical_and(posF > (pos_cent - pos_half_width), posF < (pos_cent + pos_half_width))
        X_pos[pos_ind,:] = np.mean(X[these_frames,:], axis = 0)

    # Calcuate time-binned X  
    # for tm_ind, tm_cent in enumerate(tm_centers):
    #     these_frames = np.logical_and(time_from_cho > (tm_cent - tm_half_width), time_from_cho < (tm_cent + tm_half_width))
    #     X_tm[tm_ind,:] = np.mean(X[these_frames,:], axis = 0)

    return X_pos